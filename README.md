# üìö Personalized Study Assistant with Local RAG

## Project Overview

This project develops an interactive web application, built with **Streamlit**, that functions as a personalized study assistant. It leverages **Retrieval-Augmented Generation (RAG)** to provide accurate, context-aware answers to user questions based on uploaded study documents.

A key differentiator of this project is its commitment to **local execution**. All Large Language Models (LLMs) and embedding models run directly on the user's machine via **Ollama**, ensuring data privacy and eliminating the need for costly cloud API services.

## ‚ú® Key Features

* **Document Ingestion:** Upload diverse study materials, including PDF, TXT, and Markdown files.
* **Local Knowledge Base:** Documents are processed and stored in a persistent **ChromaDB** vector database locally on your machine. New uploads add to the existing knowledge base.
* **Local LLM & Embeddings:** Utilizes local models managed by [Ollama](https://ollama.com/):
    * **Embeddings:** `nomic-embed-text` for converting text into numerical representations.
    * **Generative Model:** `gemma3:12b-it` (or your chosen instruction-tuned model) for generating answers.
* **Retrieval-Augmented Generation (RAG):** Answers are intelligently generated by first retrieving relevant information from your uploaded documents, then using the LLM to synthesize a concise and accurate response based *only* on that context.
* **Intuitive User Interface:** A user-friendly web interface developed with **Streamlit** for seamless interaction.
* **Privacy-Focused:** All data processing and AI inference occur locally, ensuring your study materials never leave your device.

## üöÄ How It Works

1.  **Upload:** Users upload their study documents (PDFs, TXT, MD) via the Streamlit interface.
2.  **Process:** The application loads these documents, splits them into manageable chunks, and converts them into numerical embeddings using the local `nomic-embed-text` model.
3.  **Store:** These embeddings, along with the original text chunks, are stored in a local ChromaDB vector database.
4.  **Query:** When a user asks a question, the application converts the question into an embedding.
5.  **Retrieve:** It then queries the ChromaDB to find the most semantically similar text chunks from the uploaded documents.
6.  **Generate:** These retrieved chunks are sent as context to the local `gemma3:12b-it` LLM, which then generates an answer based *only* on the provided context.

## ‚öôÔ∏è Setup & Running the Application

Follow these steps to get your Personalized Study Assistant up and running on your local machine.

### Prerequisites

Before you begin, ensure you have the following installed:

* **Python 3.9+** (recommended)
* **`pip`** (Python package installer, usually comes with Python)
* **`git`** (for cloning the repository)
* **Ollama:** Download and install Ollama from [ollama.com/download](https://ollama.com/download). This is crucial as it runs your LLM and embedding models locally.

### Step-by-Step Guide

1.  **Clone the Repository:**
    Open your terminal or command prompt and clone the project repository:

    ```bash
    git clone [https://github.com/your-username/Study_Assistant_RAG.git](https://github.com/your-username/Study_Assistant_RAG.git)
    cd Study_Assistant_RAG
    ```
    *(Remember to replace `your-username` with your actual GitHub username)*

2.  **Set up a Python Virtual Environment:**
    It's best practice to use a virtual environment to manage project dependencies.

    ```bash
    python -m venv venv
    source venv/bin/activate
    # On Windows, use: venv\Scripts\activate
    ```

3.  **Install Python Dependencies:**
    With your virtual environment activated, install all the necessary Python libraries:

    ```bash
    pip install -r requirements.txt
    # Ensure these specific libraries are installed/updated for best compatibility:
    pip install -U langchain-ollama langchain-chroma cryptography
    ```
    *(Note: `cryptography` is essential for proper PDF document handling.)*

4.  **Pull Ollama Models:**
    Ensure your Ollama server is running in the background (it typically starts automatically after installation). Then, download the required LLM and embedding models:

    ```bash
    ollama pull gemma3:12b-it
    ollama pull nomic-embed-text
    ```
    *(You can run `ollama list` in your terminal to verify that the models have been successfully downloaded.)*

5.  **Run the Streamlit Application:**
    Finally, launch the application:

    ```bash
    streamlit run streamlit_app.py
    ```
    This command will open the Personalized Study Assistant in your default web browser, usually at `http://localhost:8501`.

---